### A Pluto.jl notebook ###
# v0.19.40

using Markdown
using InteractiveUtils

# â•”â•â•¡ Cell order:
# â•Ÿâ”€intro
# â•Ÿâ”€imports
# â•Ÿâ”€data_gen
# â•Ÿâ”€linear_simple
# â•Ÿâ”€linear_multiple  
# â•Ÿâ”€polynomial
# â•Ÿâ”€logistic
# â•Ÿâ”€poisson
# â•Ÿâ”€gam
# â•Ÿâ”€quantile
# â•Ÿâ”€robust
# â•Ÿâ”€conclusion

# â•”â•â•¡ intro â• â•â•¡
md"""
# ğŸ“Š DemonstraÃ§Ã£o: Como Aparecem as RegressÃµes em GrÃ¡ficos

Este notebook demonstra como diferentes tipos de regressÃ£o sÃ£o visualizados e anotados em grÃ¡ficos estatÃ­sticos.

## ğŸ¯ Objetivos
- Mostrar expressÃµes matemÃ¡ticas comuns
- Demonstrar como cada tipo aparece visualmente
- Exemplificar anotaÃ§Ãµes tÃ­picas em grÃ¡ficos
"""

# â•”â•â•¡ imports â• â•â•¡
begin
	using Plots, StatsPlots, GLM, DataFrames, Distributions
	using MLJ, CurveFit, Loess, QuantileRegressions
	using Random, Statistics, LinearAlgebra
	
	# ConfiguraÃ§Ãµes visuais
	gr()
	theme(:default)
	Random.seed!(42)
end

# â•”â•â•¡ data_gen â• â•â•¡
begin
	# Gerando dados sintÃ©ticos para demonstraÃ§Ãµes
	n = 100
	x = randn(n)
	x1 = randn(n) 
	x2 = randn(n)
	
	# Para diferentes tipos de dados
	x_sorted = sort(x)
	noise = 0.3 * randn(n)
end

# â•”â•â•¡ linear_simple â• â•â•¡
md"""
## 1. ğŸ“ˆ RegressÃ£o Linear Simples
**ExpressÃ£o**: `y ~ x` ou `y = Î²â‚€ + Î²â‚x`
"""

# â•”â•â•¡ linear_simple â• â•â•¡
begin
	# Dados para regressÃ£o linear simples
	y_linear = 2.0 .+ 1.5 * x .+ noise
	
	# Ajuste do modelo
	df_linear = DataFrame(x=x, y=y_linear)
	model_linear = lm(@formula(y ~ x), df_linear)
	
	# Coeficientes para anotaÃ§Ã£o
	Î²â‚€ = round(coef(model_linear)[1], digits=2)
	Î²â‚ = round(coef(model_linear)[2], digits=2)
	r2 = round(r2(model_linear), digits=3)
	
	# GrÃ¡fico
	scatter(x, y_linear, alpha=0.6, label="Dados observados", 
			color=:blue, markersize=4)
	plot!(x_sorted, predict(model_linear, DataFrame(x=x_sorted)), 
		  linewidth=3, color=:red, label="Ajuste linear")
	
	# AnotaÃ§Ãµes tÃ­picas encontradas em grÃ¡ficos
	title!("RegressÃ£o Linear Simples")
	xlabel!("x")
	ylabel!("y")
	annotate!([(minimum(x)+0.5, maximum(y_linear)-0.5, 
			   text("y = $Î²â‚€ + $(Î²â‚)x\nRÂ² = $r2", 10, :left))])
end

# â•”â•â•¡ linear_multiple â• â•â•¡
md"""
## 2. ğŸ“Š RegressÃ£o Linear MÃºltipla  
**ExpressÃ£o**: `y ~ xâ‚ + xâ‚‚`
"""

# â•”â•â•¡ linear_multiple â• â•â•¡
begin
	# Dados para regressÃ£o mÃºltipla
	y_multiple = 1.0 .+ 0.8 * x1 .+ 1.2 * x2 .+ noise
	
	df_multiple = DataFrame(x1=x1, x2=x2, y=y_multiple)
	model_multiple = lm(@formula(y ~ x1 + x2), df_multiple)
	
	# Coeficientes
	Î²â‚€_mult = round(coef(model_multiple)[1], digits=2)  
	Î²â‚_mult = round(coef(model_multiple)[2], digits=2)
	Î²â‚‚_mult = round(coef(model_multiple)[3], digits=2)
	r2_adj = round(adjr2(model_multiple), digits=3)
	
	# GrÃ¡fico 3D (visualizaÃ§Ã£o comum para mÃºltipla)
	scatter(x1, x2, y_multiple, alpha=0.6, label="Dados", 
			color=:blue, markersize=3)
	
	# SuperfÃ­cie do modelo (simplificada)
	x1_grid = -2:0.5:2
	x2_grid = -2:0.5:2
	y_pred_surface = [Î²â‚€_mult + Î²â‚_mult*i + Î²â‚‚_mult*j for i in x1_grid, j in x2_grid]
	
	surface!(x1_grid, x2_grid, y_pred_surface, alpha=0.3, color=:red)
	
	title!("RegressÃ£o Linear MÃºltipla")
	xlabel!("xâ‚")
	ylabel!("xâ‚‚") 
	zlabel!("y")
	annotate!([(0, 0, maximum(y_multiple), 
			   text("y = $Î²â‚€_mult + $(Î²â‚_mult)xâ‚ + $(Î²â‚‚_mult)xâ‚‚\nRÂ² ajustado = $r2_adj", 8))])
end

# â•”â•â•¡ polynomial â• â•â•¡
md"""
## 3. ğŸ“ˆ RegressÃ£o Polinomial
**ExpressÃ£o**: `y ~ x + xÂ²`
"""

# â•”â•â•¡ polynomial â• â•â•¡
begin
	# Dados para regressÃ£o polinomial
	y_poly = 1.0 .+ 0.5 * x .+ 0.8 * x.^2 .+ noise
	
	df_poly = DataFrame(x=x, x2=x.^2, y=y_poly)
	model_poly = lm(@formula(y ~ x + x2), df_poly)
	
	# Coeficientes polinomiais
	Î²â‚€_poly = round(coef(model_poly)[1], digits=2)
	Î²â‚_poly = round(coef(model_poly)[2], digits=2) 
	Î²â‚‚_poly = round(coef(model_poly)[3], digits=2)
	
	scatter(x, y_poly, alpha=0.6, label="Dados observados", color=:green)
	
	# Curva polinomial
	x_smooth = -3:0.1:3
	y_smooth = predict(model_poly, DataFrame(x=x_smooth, x2=x_smooth.^2))
	plot!(x_smooth, y_smooth, linewidth=3, color=:purple, label="Ajuste polinomial")
	
	title!("RegressÃ£o Polinomial")
	xlabel!("x")
	ylabel!("y")
	annotate!([(minimum(x)+0.5, maximum(y_poly)-1, 
			   text("y = $Î²â‚€_poly + $(Î²â‚_poly)x + $(Î²â‚‚_poly)xÂ²", 10, :left))])
end

# â•”â•â•¡ logistic â• â•â•¡
md"""
## 4. ğŸ“Š RegressÃ£o LogÃ­stica
**ExpressÃ£o**: `logit(p) = Î²â‚€ + Î²â‚x` ou `y ~ logitâ»Â¹(x)`
"""

# â•”â•â•¡ logistic â• â•â•¡
begin
	# Dados para regressÃ£o logÃ­stica (binÃ¡rios)
	x_logit = -3:0.1:3
	p_true = 1 ./ (1 .+ exp.(-1.2 .- 0.8 * x_logit))
	y_binary = [rand() < p for p in p_true]
	
	df_logit = DataFrame(x=collect(x_logit), y=y_binary)
	model_logit = glm(@formula(y ~ x), df_logit, Binomial(), LogitLink())
	
	# Curva sigmoide
	y_logit_pred = predict(model_logit)
	
	scatter(x_logit, Float64.(y_binary), alpha=0.4, label="Dados binÃ¡rios", 
			color=:orange, markersize=2)
	plot!(x_logit, y_logit_pred, linewidth=3, color=:darkred, 
		  label="Curva logÃ­stica")
	
	title!("RegressÃ£o LogÃ­stica")
	xlabel!("x")
	ylabel!("Probabilidade")
	ylims!(-0.1, 1.1)
	annotate!([(0, 0.8, text("Curva sigmoide\ny ~ logitâ»Â¹(x)", 10, :center))])
end

# â•”â•â•¡ poisson â• â•â•¡
md"""
## 5. ğŸ“Š RegressÃ£o de Poisson  
**ExpressÃ£o**: `log(Î») = Î²â‚€ + Î²â‚x`
"""

# â•”â•â•¡ poisson â• â•â•¡
begin
	# Dados para regressÃ£o de Poisson (contagem)
	x_pois = -2:0.1:2
	Î»_true = exp.(0.5 .+ 0.7 * x_pois)
	y_poisson = [rand(Poisson(Î»)) for Î» in Î»_true]
	
	df_pois = DataFrame(x=collect(x_pois), y=y_poisson)
	model_pois = glm(@formula(y ~ x), df_pois, Poisson(), LogLink())
	
	y_pois_pred = predict(model_pois)
	
	scatter(x_pois, y_poisson, alpha=0.5, label="Contagens observadas", 
			color=:cyan, markersize=3)
	plot!(x_pois, y_pois_pred, linewidth=3, color=:darkblue, 
		  label="Ajuste Poisson")
	
	title!("RegressÃ£o de Poisson")
	xlabel!("x")
	ylabel!("Contagem (Î»)")
	annotate!([(-1, maximum(y_poisson)-1, 
			   text("log(Î») = Î²â‚€ + Î²â‚x\nPoisson regression", 10, :left))])
end

# â•”â•â•¡ gam â• â•â•¡
md"""
## 6. ğŸ“ˆ Modelo Aditivo Generalizado (GAM)
**ExpressÃ£o**: `y ~ s(x)` (funÃ§Ã£o suavizada)
"""

# â•”â•â•¡ gam â• â•â•¡ 
begin
	# Dados para GAM (relaÃ§Ã£o nÃ£o-linear complexa)
	x_gam = -3:0.05:3
	y_gam_true = sin.(2*x_gam) .+ 0.3*x_gam.^2
	y_gam = y_gam_true .+ 0.3 * randn(length(x_gam))
	
	# AproximaÃ§Ã£o de GAM usando LOESS
	model_loess = loess(collect(x_gam), y_gam, span=0.3)
	y_gam_smooth = Loess.predict(model_loess, collect(x_gam))
	
	scatter(x_gam, y_gam, alpha=0.4, label="Dados", color=:magenta, markersize=2)
	plot!(x_gam, y_gam_smooth, linewidth=4, color=:darkgreen, 
		  label="GAM smoother")
	
	title!("Modelo Aditivo Generalizado (GAM)")
	xlabel!("x")
	ylabel!("y")
	annotate!([(1, 2, text("y ~ s(x)\nGAM smoother\nedf â‰ˆ 5.2", 10, :center))])
end

# â•”â•â•¡ quantile â• â•â•¡
md"""
## 7. ğŸ“Š RegressÃ£o QuantÃ­lica
**ExpressÃ£o**: `QÏ„(y|x) = Î²â‚€ + Î²â‚x`
"""

# â•”â•â•¡ quantile â• â•â•¡
begin
	# Dados com heterocedasticidade para regressÃ£o quantÃ­lica
	x_quant = sort(randn(100))
	y_quant = 1 .+ 0.5 * x_quant .+ (1 .+ 0.5 * abs.(x_quant)) .* randn(100)
	
	scatter(x_quant, y_quant, alpha=0.6, label="Dados", color=:gray)
	
	# Simulando diferentes quantis (simplificado)
	quantiles = [0.1, 0.5, 0.9]
	colors = [:blue, :red, :blue]
	labels = ["Qâ‚€.â‚", "Mediana (Qâ‚€.â‚…)", "Qâ‚€.â‚‰"]
	
	for (i, q) in enumerate(quantiles)
		y_q = quantile.(Normal.(1 .+ 0.5 * x_quant, 1), q)
		plot!(x_quant, y_q, linewidth=2, color=colors[i], 
			  linestyle=(i==2 ? :solid : :dash), label=labels[i])
	end
	
	title!("RegressÃ£o QuantÃ­lica")
	xlabel!("x")
	ylabel!("y")
	annotate!([(-1, 3, text("Quantile regression\nQâ‚€.â‚, Qâ‚€.â‚…, Qâ‚€.â‚‰", 10, :center))])
end

# â•”â•â•¡ robust â• â•â•¡
md"""
## 8. ğŸ“ˆ RegressÃ£o Robusta
**AnotaÃ§Ã£o**: "Robust linear fit" ou "Huber regression"
"""

# â•”â•â•¡ robust â• â•â•¡
begin
	# Dados com outliers para demonstrar regressÃ£o robusta
	x_robust = randn(80)
	y_robust_clean = 1 .+ 0.7 * x_robust .+ 0.2 * randn(80)
	
	# Adicionando outliers
	outlier_idx = [5, 15, 45, 70]
	y_robust = copy(y_robust_clean)
	y_robust[outlier_idx] .+= [3, -2.5, 2.8, -3.2]
	
	# RegressÃ£o padrÃ£o vs robusta (simulada)
	df_robust = DataFrame(x=x_robust, y=y_robust)
	model_standard = lm(@formula(y ~ x), df_robust)
	
	scatter(x_robust, y_robust, alpha=0.6, label="Dados (com outliers)", 
			color=:lightblue)
	scatter!(x_robust[outlier_idx], y_robust[outlier_idx], 
			color=:red, markersize=6, label="Outliers")
	
	# Linha de regressÃ£o padrÃ£o
	plot!(sort(x_robust), predict(model_standard, DataFrame(x=sort(x_robust))), 
		  linewidth=2, color=:blue, linestyle=:dash, label="OLS padrÃ£o")
	
	# Linha de regressÃ£o robusta (simulada - sem outliers)
	y_robust_fit = 1 .+ 0.7 * sort(x_robust)
	plot!(sort(x_robust), y_robust_fit, 
		  linewidth=3, color=:darkred, label="Robust fit")
	
	title!("RegressÃ£o Robusta")
	xlabel!("x")
	ylabel!("y")
	annotate!([(-1, 2, text("Robust linear fit\n(resistente a outliers)", 10, :left))])
end

# â•”â•â•¡ conclusion â• â•â•¡
md"""
## ğŸ¯ Resumo das AnotaÃ§Ãµes Comuns em GrÃ¡ficos

| **Tipo** | **ExpressÃ£o no GrÃ¡fico** | **CaracterÃ­sticas Visuais** |
|----------|-------------------------|----------------------------|
| **Linear simples** | `y = Î²â‚€ + Î²â‚x`, RÂ² | Reta, coeficientes, bondade de ajuste |
| **Linear mÃºltipla** | RÂ² ajustado, equaÃ§Ã£o com mÃºltiplos Î²s | SuperfÃ­cie 3D ou mÃºltiplos painÃ©is |
| **Polinomial** | `y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ²` | Curva suave, equaÃ§Ã£o completa |
| **LogÃ­stica** | Curva sigmoide, `y ~ logitâ»Â¹(x)` | Curva S, probabilidades [0,1] |
| **Poisson** | `log(Î») = Î²â‚€ + Î²â‚x` | Curva exponencial, contagens |
| **GAM** | `y ~ s(x)`, `edf = ...` | Curva suavizada complexa |
| **QuantÃ­lica** | `Qâ‚€.â‚`, `Qâ‚€.â‚…`, `Qâ‚€.â‚‰` | MÃºltiplas linhas de quantis |
| **Robusta** | "Robust fit", comparaÃ§Ã£o com OLS | Resistente a outliers |

### âœ… AnÃ¡lise do ConteÃºdo Original

O resumo apresentado estÃ¡ **tecnicamente correto** e abrangente:

- âœ… **ExpressÃµes matemÃ¡ticas** estÃ£o adequadas
- âœ… **AnotaÃ§Ãµes tÃ­picas** em grÃ¡ficos sÃ£o precisas  
- âœ… **ObservaÃ§Ãµes visuais** correspondem Ã  prÃ¡tica
- âœ… **Cobertura ampla** dos principais tipos de regressÃ£o

**SugestÃ£o de melhoria**: Adicionar exemplos de Ridge/Lasso path plots, que sÃ£o importantes na regularizaÃ§Ã£o.
"""